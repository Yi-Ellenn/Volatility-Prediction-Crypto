{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练数据格式\n",
    "LightGBM 支持 CSV, TSV 和 LibSVM 格式的输入数据文件。\n",
    "Label 是第一列的数据，文件中是不包含 header（标题） 的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类别特征支持\n",
    "LightGBM 可以直接使用 categorical feature（类别特征）（不需要单独编码）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train) #将数据保存到lightGBM二进制文件将使加载更快\n",
    "lgb_eval = lgb.Dataset(X_test, y_test) #创建验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task':'train',\n",
    "    'boosting_type':'gbdt', #设置提升类型\n",
    "    'objective':'regression', #目标函数\n",
    "    'metric':{'l2','auc'}, #评估函数\n",
    "    'num_leaves':31,  #叶子节点数\n",
    "    'learning_rate':0.05, #学习速率\n",
    "    'feature_fraction':0.9, #建树的特征选择比例\n",
    "    'bagging_fraction':0.8, #建树的样本采样比例\n",
    "    'bagging_freq':5, # k 意味着每k次迭代执行bagging\n",
    "    'verbose':1 # <0 显示致命的； =0 显示错误（警告）； >0 显示信息\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87\n",
      "[LightGBM] [Info] Number of data points in the train set: 120, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 1.008333\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\ttraining's l2: 0.644495\ttraining's auc: 0.99359\tvalid_1's l2: 0.455037\tvalid_1's auc: 0.954545\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\ttraining's l2: 0.586588\ttraining's auc: 0.99359\tvalid_1's l2: 0.413128\tvalid_1's auc: 0.954545\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\ttraining's l2: 0.534155\ttraining's auc: 0.99359\tvalid_1's l2: 0.375581\tvalid_1's auc: 0.954545\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\ttraining's l2: 0.486845\ttraining's auc: 0.99359\tvalid_1's l2: 0.341896\tvalid_1's auc: 0.954545\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\ttraining's l2: 0.443975\ttraining's auc: 0.99359\tvalid_1's l2: 0.311748\tvalid_1's auc: 0.954545\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\ttraining's l2: 0.404842\ttraining's auc: 0.99359\tvalid_1's l2: 0.283838\tvalid_1's auc: 0.954545\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 0.644495\ttraining's auc: 0.99359\tvalid_1's l2: 0.455037\tvalid_1's auc: 0.954545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/product_zhpu/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "gbm = lgb.train(params,lgb_train, num_boost_round=20,valid_sets=[lgb_train, lgb_eval], early_stopping_rounds=5) # 训练数据需要参数列表和数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f96187d6e20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.save_model('model.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of prediction is: 0.6745642146042917\n"
     ]
    }
   ],
   "source": [
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5) # 计算真实值和预测值之间的均方根误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95938726, 0.95938726, 1.05278846, 1.05278846, 0.95938726,\n",
       "       1.00983974, 1.00983974, 0.95938726, 1.05278846, 1.00983974,\n",
       "       1.00983974, 0.95938726, 0.95938726, 0.95938726, 1.05278846,\n",
       "       1.00983974, 1.05278846, 1.00983974, 1.00983974, 1.05278846,\n",
       "       1.00983974, 1.05278846, 1.00983974, 1.00983974, 0.95938726,\n",
       "       0.95938726, 0.95938726, 1.05278846, 1.00983974, 1.00983974])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 0, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2,\n",
       "       1, 1, 0, 0, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 88\n",
      "[LightGBM] [Info] Number of data points in the train set: 120, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.983333\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\ttraining's l2: 0.605777\ttraining's auc: 0.987342\tvalid_1's l2: 0.608945\tvalid_1's auc: 0.97619\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\ttraining's l2: 0.551047\ttraining's auc: 0.987342\tvalid_1's l2: 0.554966\tvalid_1's auc: 0.97619\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\ttraining's l2: 0.501629\ttraining's auc: 0.987342\tvalid_1's l2: 0.506601\tvalid_1's auc: 0.97619\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\ttraining's l2: 0.455715\ttraining's auc: 0.999691\tvalid_1's l2: 0.463858\tvalid_1's auc: 0.997354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\ttraining's l2: 0.413831\ttraining's auc: 0.999691\tvalid_1's l2: 0.423967\tvalid_1's auc: 0.997354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\ttraining's l2: 0.376126\ttraining's auc: 0.999691\tvalid_1's l2: 0.388316\tvalid_1's auc: 0.997354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\ttraining's l2: 0.342107\ttraining's auc: 0.999691\tvalid_1's l2: 0.35639\tvalid_1's auc: 0.997354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\ttraining's l2: 0.311413\ttraining's auc: 0.999691\tvalid_1's l2: 0.327814\tvalid_1's auc: 0.997354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\ttraining's l2: 0.283722\ttraining's auc: 0.999691\tvalid_1's l2: 0.302249\tvalid_1's auc: 0.997354\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttraining's l2: 0.455715\ttraining's auc: 0.999691\tvalid_1's l2: 0.463858\tvalid_1's auc: 0.997354\n",
      "Save model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/product_zhpu/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "# 创建成lgb的数据集格式\n",
    "lgb_train = lgb.Dataset(X_train, y_train) #将数据保存到lightGBM二进制文件将使加载更快\n",
    "lgb_eval = lgb.Dataset(X_test, y_test) #创建验证数据\n",
    "\n",
    "#将参数写成字典形式\n",
    "params = {\n",
    "    'task':'train',\n",
    "    'boosting_type':'gbdt', #设置提升类型\n",
    "    'objective':'regression', #目标函数\n",
    "    'metric':{'l2','auc'}, #评估函数\n",
    "    'num_leaves':31,  #叶子节点数\n",
    "    'learning_rate':0.05, #学习速率\n",
    "    'feature_fraction':0.9, #建树的特征选择比例\n",
    "    'bagging_fraction':0.8, #建树的样本采样比例\n",
    "    'bagging_freq':5, # k 意味着每k次迭代执行bagging\n",
    "    'verbose':1 # <0 显示致命的； =0 显示错误（警告）； >0 显示信息\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# 训练 cv and train\n",
    "gbm = lgb.train(params,lgb_train, num_boost_round=20,valid_sets=[lgb_train, lgb_eval],early_stopping_rounds=5) # 训练数据需要参数列表和数据集\n",
    "print('Save model...') \n",
    "gbm.save_model('model.txt') \n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17191864, 0.80939951, 0.80939951, 1.17191864, 1.17191864,\n",
       "       0.99632255, 0.85422191, 0.99632255, 0.85422191, 0.99632255,\n",
       "       0.80939951, 0.80939951, 1.17191864, 0.99632255, 1.17191864,\n",
       "       0.80939951, 1.17191864, 1.17191864, 1.17191864, 0.99632255,\n",
       "       0.99632255, 1.17191864, 0.99632255, 0.80939951, 1.17191864,\n",
       "       1.17191864, 0.80939951, 0.99632255, 0.80939951, 1.17191864])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zehoo/Documents/Course/个人课程/PRG_波动率预测/code/Lecture_3/lgb_demo.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zehoo/Documents/Course/%E4%B8%AA%E4%BA%BA%E8%AF%BE%E7%A8%8B/PRG_%E6%B3%A2%E5%8A%A8%E7%8E%87%E9%A2%84%E6%B5%8B/code/Lecture_3/lgb_demo.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe rmse of prediction is:\u001b[39m\u001b[39m'\u001b[39m, mean_squared_error(y_test, y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5) # 计算真实值和预测值之间的均方根误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Dataset at 0x7f9d88dfcb20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 86\n",
      "[LightGBM] [Info] Number of data points in the train set: 120, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.958333\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's auc: 0.977273\tvalid_0's l2: 0.6505\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's auc: 0.977273\tvalid_0's l2: 0.591663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's auc: 0.997159\tvalid_0's l2: 0.539564\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's auc: 1\tvalid_0's l2: 0.493325\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's auc: 1\tvalid_0's l2: 0.45178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's auc: 1\tvalid_0's l2: 0.412135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's auc: 1\tvalid_0's l2: 0.378125\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's auc: 1\tvalid_0's l2: 0.343605\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's auc: 1\tvalid_0's l2: 0.315658\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 1\tvalid_0's l2: 0.493325\n",
      "Save model...\n",
      "Start predicting...\n",
      "The rmse of prediction is: 0.7023709058255572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/product_zhpu/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "iris = load_iris() #载入鸢尾花数据集\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "# 加载你的数据\n",
    "# print('Load data...')\n",
    "# df_train = pd.read_csv('../regression/regression.train', header=None, sep='\\t')\n",
    "# df_test = pd.read_csv('../regression/regression.test', header=None, sep='\\t')\n",
    "#\n",
    "# y_train = df_train[0].values\n",
    "# y_test = df_test[0].values\n",
    "# X_train = df_train.drop(0, axis=1).values\n",
    "# X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "# 创建成lgb的数据集格式\n",
    "lgb_train = lgb.Dataset(X_train, y_train) #将数据保存到lightGBM二进制文件将使加载更快\n",
    "lgb_eval = lgb.Dataset(X_test, y_test) #创建验证数据\n",
    "\n",
    "#将参数写成字典形式\n",
    "params = {\n",
    "    'task':'train',\n",
    "    'boosting_type':'gbdt', #设置提升类型\n",
    "    'objective':'regression', #目标函数\n",
    "    'metric':{'l2','auc'}, #评估函数\n",
    "    'num_leaves':31,  #叶子节点数\n",
    "    'learning_rate':0.05, #学习速率\n",
    "    'feature_fraction':0.9, #建树的特征选择比例\n",
    "    'bagging_fraction':0.8, #建树的样本采样比例\n",
    "    'bagging_freq':5, # k 意味着每k次迭代执行bagging\n",
    "    'verbose':1 # <0 显示致命的； =0 显示错误（警告）； >0 显示信息\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# 训练 cv and train\n",
    "gbm = lgb.train(params,lgb_train,num_boost_round=20,valid_sets=lgb_eval,early_stopping_rounds=5) # 训练数据需要参数列表和数据集\n",
    " \n",
    "print('Save model...') \n",
    " \n",
    "gbm.save_model('model.txt')   # 训练后保存模型到文件\n",
    " \n",
    "print('Start predicting...')\n",
    "# 预测数据集\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration) #如果在训练期间启用了早期停止，可以通过best_iteration方式从最佳迭代中获得预测\n",
    "# 评估模型\n",
    "\n",
    "\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5) # 计算真实值和预测值之间的均方根误差"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product_zhpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
